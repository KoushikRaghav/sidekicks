{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing policy using method from https://arxiv.org/abs/1711.00138\n",
    "# GitHub repo: https://github.com/greydanus/visualize_atari\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import torchvision\n",
    "import tensorboardX\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as skitran\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), 'misc/'))\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "from torch.optim import lr_scheduler\n",
    "from tensorboardX import SummaryWriter\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refer https://github.com/greydanus/visualize_atari/blob/master/saliency.py\n",
    "\n",
    "def occlude(I, mask):\n",
    "    num_channels = I.shape[2]\n",
    "    I_out = np.zeros(I.shape)\n",
    "    for chn in range(num_channels):\n",
    "        I_out[:, :, chn] = I[:, :, chn]*(1-mask) + gaussian_filter(I[:, :, chn], sigma=5)*mask \n",
    "    return I_out\n",
    "    \n",
    "def get_mask(center, size, r):\n",
    "    y,x = np.ogrid[-center[0]:size[0]-center[0], -center[1]:size[1]-center[1]]\n",
    "    keep = x*x + y*y <= 1\n",
    "    mask = np.zeros(size) ; mask[keep] = 1 # select a circle of pixels\n",
    "    mask = gaussian_filter(mask, sigma=r) # blur the circle of pixels. this is a 2D Gaussian for r=r^2=1\n",
    "    return mask/mask.max()\n",
    "\n",
    "def saliency_on_image(saliency, image, fudge_factor, channels=[2], sigma=0):\n",
    "    # Refer https://github.com/greydanus/visualize_atari/blob/master/saliency.py\n",
    "    pmax = saliency.max()\n",
    "    S = saliency\n",
    "    S = S if sigma == 0 else gaussian_filter(S, sigma=sigma)\n",
    "    S -= S.min()\n",
    "    S = 255.0*fudge_factor*pmax*S/S.max()\n",
    "    I = np.copy(image.astype('uint16'))\n",
    "    if image.shape[2] == 1:\n",
    "        I[:, :, 0] += S.astype('uint16')\n",
    "    else:\n",
    "        for channel in channels:\n",
    "            I[:, :, channel] += S.astype('uint16')\n",
    "    I = I.clip(1, 255).astype('uint8')\n",
    "    return I\n",
    "\n",
    "def get_belief_saliency(belief, others, agent, iscuda, lr=1e-4, weight_decay=1e-1, iters=200):\n",
    "    \"\"\"\n",
    "    belief: current hidden state of the aggregator\n",
    "    others: dictionary of elevation('elev'), azimuth('azim'), time('time'), proprioception change('pro')\n",
    "    NOTE: Assuming batch size = 1\n",
    "          Ensure that the agent's parameters have requires_grad=False\n",
    "    \"\"\"\n",
    "    M = agent.M\n",
    "    N = agent.N\n",
    "    C = agent.C\n",
    "    d_belief_orig = Variable(torch.randn(1, 256)*1e-3, requires_grad=True)\n",
    "    optimizer = optim.SGD([d_belief_orig], lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "    \n",
    "    act_input_1 = torch.cat([belief.view(1, -1), others['pro'][:, :2]], dim=1) \n",
    "    \n",
    "    if 'elev' in others:\n",
    "        xe = others['elev']\n",
    "        act_input_1 = torch.cat([act_input_1, xe], dim=1)\n",
    "    if 'azim' in others:\n",
    "        xa = others['azim']\n",
    "        act_input_1 = torch.cat([act_input_1, xa], dim=1)\n",
    "    if 'time' in others:\n",
    "        xt = others['time']\n",
    "        act_input_1 = torch.cat([act_input_1, xt], dim=1)\n",
    "    \n",
    "    activations_1 = agent.policy.act(act_input_1)\n",
    "    # Solve for d_belief that maximises change in policy activations\n",
    "    for i in range(iters):\n",
    "        if iscuda:\n",
    "            d_belief = d_belief_orig.cuda()\n",
    "        else:\n",
    "            d_belief = d_belief_orig\n",
    "            \n",
    "        act_input_2 = torch.cat([d_belief + belief.view(1, -1), others['pro'][:, :2]], dim=1)\n",
    "        \n",
    "        if 'elev' in others:\n",
    "            xe = others['elev']\n",
    "            act_input_2 = torch.cat([act_input_2, xe], dim=1)\n",
    "        if 'azim' in others:\n",
    "            xa = others['azim']\n",
    "            act_input_2 = torch.cat([act_input_2, xa], dim=1)\n",
    "        if 'time' in others:\n",
    "            xt = others['time']\n",
    "            act_input_2 = torch.cat([act_input_2, xt], dim=1)\n",
    "\n",
    "        activations_2 = agent.policy.act(act_input_2)\n",
    "        loss = -((activations_1 - activations_2)**2).sum() + weight_decay*(d_belief**2).sum()\n",
    "        loss.backward()\n",
    "        #pdb.set_trace()\n",
    "        optimizer.step()\n",
    "        if torch.norm(d_belief_orig.data) >= 0.75 * torch.norm(belief.data):\n",
    "            break\n",
    "    diff = -((activations_1 - activations_2)**2).sum()\n",
    "    diff_probs = -((F.softmax(activations_1, dim=1) - F.softmax(activations_2, dim=1))**2).sum()\n",
    "    #pdb.set_trace()\n",
    "    #print('Final loss: %.3f, Act prob diff: %f, Belief norm: %.4f, Delta norm: %.4f, Terminated iter: %d'%\\\n",
    "    #                 (diff.data[0], diff_probs.data[0], torch.norm(belief)[0], torch.norm(d_belief_orig)[0], i)) \n",
    "    \n",
    "    # Get the saliency\n",
    "    decoded_2 = agent.policy.decode(F.normalize(d_belief + belief.view(1, -1), p=1, dim=1))\n",
    "    decoded_2 = decoded_2.view(1, N, M, C, 32, 32)\n",
    "    decoded_1 = agent.policy.decode(F.normalize(belief.view(1, -1), p=1, dim=1))\n",
    "    decoded_1 = decoded_1.view(1, N, M, C, 32, 32)\n",
    "    saliency = np.sum(0.5*((decoded_1 - decoded_2).data.cpu().numpy())**2, axis=3)\n",
    "    #saliency /= (saliency.max()+1e-9)\n",
    "    \n",
    "    return saliency, diff.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_path = '/vision/vision_users/srama/Research/LearningToLookAround/models/SUN360_models_very_old/multi_no_expert_reinforce/run1/model_best.net'\n",
    "loaded_state = torch.load(model_path)\n",
    "opts = loaded_state['opts']\n",
    "opts.batch_size = 50\n",
    "opts.critic_full_obs = False\n",
    "opts.act_full_obs = False\n",
    "\n",
    "if opts.dataset == 0:\n",
    "    opts.h5_path = '/vision/vision_users/srama/Research/LearningToLookAround/data/SUN360/data_4_plus_highres.h5'\n",
    "    \n",
    "if not hasattr(opts, 'h5_path_unseen'):\n",
    "    if opts.dataset == 0:\n",
    "        opts.h5_path_unseen = ''\n",
    "    else:\n",
    "        opts.h5_path_unseen = ' ../data/ModelNet/data_10.h5'\n",
    "        \n",
    "if opts.expert_trajectories:\n",
    "    opts.T_sup = 3\n",
    "\n",
    "if opts.expert_trajectories:\n",
    "    agent = AgentSupervised(opts)\n",
    "else:\n",
    "    agent = Agent(opts)\n",
    "    \n",
    "agent.policy.load_state_dict(loaded_state['state_dict'])\n",
    "agent.policy.eval()\n",
    "\n",
    "opts.expert_rewards = False\n",
    "opts.expert_trajectories = False\n",
    "\n",
    "from DataLoader import DataLoaderSimple as DataLoader\n",
    "\n",
    "loader = DataLoader(opts)\n",
    "\n",
    "set_random_seeds(opts.seed)\n",
    "\n",
    "if opts.dataset == 0:\n",
    "    opts.num_channels = 3\n",
    "    if opts.mean_subtract:\n",
    "        # R, G, B means and stds\n",
    "        opts.mean = [119.16, 107.68, 95.12]\n",
    "        opts.std = [61.88, 61.72, 67.24]\n",
    "    else:\n",
    "        opts.mean = [0, 0, 0]\n",
    "        opts.std = [1, 1, 1]\n",
    "elif opts.dataset == 1:\n",
    "    opts.num_channels = 1\n",
    "    if opts.mean_subtract:\n",
    "        # R, G, B means and stds\n",
    "        opts.mean = [193.0162338615919]\n",
    "        opts.std = [37.716024486312811]\n",
    "    else:\n",
    "        opts.mean = [0]\n",
    "        opts.std = [1]\n",
    "else:\n",
    "    raise ValueError('Dataset %d does not exist!'%(opts.dataset))\n",
    "    \n",
    "# Avoid computing gradients for the agent in backward\n",
    "for parameter in agent.policy.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     97,
     138,
     144,
     176,
     190,
     202,
     214,
     229
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Disable this if no need for highres\n",
    "opts.highres = False\n",
    "\n",
    "saliency_scores_all = []\n",
    "saliency_belief_all = []\n",
    "images_all = []\n",
    "pano_all = []\n",
    "if opts.highres:\n",
    "    images_all_highres = []\n",
    "    \n",
    "decoded_all = []\n",
    "decoded_saliency_all = []\n",
    "decoded_all_raw = []\n",
    "\n",
    "num_iters = 4\n",
    "\n",
    "for iters in range(num_iters):\n",
    "    if opts.highres:\n",
    "        pano, pano_highres, _, depleted = loader.next_batch_test(highres=True)\n",
    "    else:\n",
    "        pano, _, depleted = loader.next_batch_test(highres=False)\n",
    "    \n",
    "    pano_rewards = None\n",
    "    pano_maps = None\n",
    "    \n",
    "    batch_size = opts.batch_size\n",
    "    start_idx = get_starts(opts.N, opts.M, batch_size, opts.start_view)\n",
    "    state_object = State(pano, pano_rewards, start_idx, opts)\n",
    "    hidden = None\n",
    "    # Collect data to display\n",
    "    images_all.append([])\n",
    "    saliency_scores_all.append([])\n",
    "    decoded_all.append([])\n",
    "    saliency_belief_all.append([])\n",
    "    decoded_all_raw.append([])\n",
    "    visited_idxes = []\n",
    "    decoded_saliency_all.append([])    \n",
    "    if opts.highres:\n",
    "        images_all_highres.append([])\n",
    "        \n",
    "    # Get the panoramas\n",
    "    pano_full_view = np.copy(state_object.views)\n",
    "    if opts.highres:\n",
    "        pano_full_view_highres = np.copy(pano_highres)\n",
    "    for i in range(batch_size):\n",
    "        if opts.num_channels == 3:\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :3, :] = 255\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :, :3] = 255\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, -3:, :] = 255\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :, -3:] = 255\n",
    "\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 1:, :3, :] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 1:, :, :3] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 1:, -3:, :] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 1:, :, -3:] = 0\n",
    "           \n",
    "        else:\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :3, :] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :, :3] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, -3:, :] = 0\n",
    "            pano_full_view[i, state_object.start_idx[i][0], state_object.start_idx[i][1], 0, :, -3:] = 0\n",
    "            \n",
    "    pano_full_view = pano_full_view.transpose((0, 3, 1, 4, 2, 5)).reshape(\\\n",
    "                                batch_size, 1, opts.num_channels, opts.N*32, opts.M*32)\n",
    "    pano_all.append(pano_full_view)\n",
    "    \n",
    "    for t in range(opts.T):\n",
    "        # Get the original image's action distribution\n",
    "        im_np, pro = state_object.get_view() # im_np - BxCx32x32\n",
    "        if opts.highres:\n",
    "            state_idx = state_object.idx\n",
    "            # use python advanced indexing to get high resolution images of current views\n",
    "            im_np_highres = pano_highres[range(len(state_idx)), [idx_[0] for idx_ in state_idx], \\\n",
    "                                                                [idx_[1] for idx_ in state_idx]]\n",
    "            \n",
    "        im, pro = preprocess(im_np, pro)\n",
    "        \n",
    "        # Store the visited locations\n",
    "        visited_idxes.append(state_object.idx)\n",
    "        \n",
    "        C = im.shape[1]\n",
    "        W = im.shape[2]\n",
    "        H = im.shape[3]\n",
    "\n",
    "        policy_input = {'im': im, 'pro': pro}\n",
    "        # Assuming only actOnElev\n",
    "        policy_input['elev'] = torch.Tensor([[state_object.idx[i][0]] for i in range(batch_size)])\n",
    "        policy_input['time'] = torch.Tensor([[t] for i in range(batch_size)])\n",
    "        if opts.iscuda:\n",
    "            for var in policy_input:\n",
    "                policy_input[var] = policy_input[var].cuda()\n",
    "        \n",
    "        for var in policy_input:\n",
    "            policy_input[var] = Variable(policy_input[var])\n",
    "        \n",
    "        probs, decoded, hidden_new, value = agent.policy.forward(policy_input, hidden)\n",
    "        saliency_scores = np.zeros((batch_size, H, W))\n",
    "        # ============ Perturb the image at different pixel locations ============ \n",
    "        for loc_x in range(0, W):\n",
    "            for loc_y in range(0, H):\n",
    "                \n",
    "                mask = get_mask(center=[loc_y,loc_x], size=[32,32], r=3)\n",
    "                \n",
    "                # To check if mask is working\n",
    "                #if loc_y == H // 2:\n",
    "                #if loc_x % 4 == 0 and loc_y % 4 == 0:\n",
    "                #    plt.imshow(mask)\n",
    "                #    plt.show()\n",
    "                #pdb.set_trace()\n",
    "                \n",
    "                im_ptbd_all = np.zeros(im_np.shape)\n",
    "                for i in range(batch_size):\n",
    "                    im_ptbd = occlude(np.transpose(im_np[i], (1, 2, 0)),mask)\n",
    "                    im_ptbd_all[i] = np.transpose(im_ptbd, (2, 0, 1))\n",
    "                policy_input['im'] = Variable(torch.Tensor(im_ptbd_all))\n",
    "                if opts.iscuda:\n",
    "                    policy_input['im'] = policy_input['im'].cuda()\n",
    "                \n",
    "                probs_x_y, _, _, value_x_y = agent.policy.forward(policy_input, hidden)\n",
    "                saliency_scores[:, loc_y, loc_x] = (probs-probs_x_y).pow(2).sum(dim=1).cpu().data.numpy()\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            saliency_scores[i] /= (saliency_scores[i].max() + 1e-10)\n",
    "        im_np_orig = np.array(im_np)*255.0\n",
    "        for chn in range(len(opts.mean)):\n",
    "            im_np_orig[:, chn] += opts.mean[chn]\n",
    "            \n",
    "        images_all[iters].append(im_np_orig)\n",
    "        saliency_scores_all[iters].append(saliency_scores)\n",
    "        if opts.highres:\n",
    "        # Apply scaled up saliency scores on the high resolution images\n",
    "            for i in range(batch_size):\n",
    "                saliency_score_highres = skitran.resize(saliency_scores[i], (448, 448))\n",
    "                im_np_curr = np.transpose(im_np_highres[i], (1, 2, 0))\n",
    "                im_np_curr = saliency_on_image(saliency_score_highres, im_np_curr, 1, \\\n",
    "                                                                             channels=[0, 2], sigma=2)\n",
    "                im_np_highres[i] = np.transpose(im_np_curr, (2, 0, 1))\n",
    "\n",
    "            images_all_highres[iters].append(im_np_highres)    \n",
    "            \n",
    "        # Create the decoded image with saliency modified view filled in\n",
    "        decoded_images = decoded.data.cpu().numpy()*255.0\n",
    "        # Add the means\n",
    "        for chn in range(len(opts.mean)):\n",
    "            decoded_images[:, :, :, chn] += opts.mean[chn]\n",
    "            \n",
    "        decoded_images_shifted = np.zeros(decoded_images.shape)\n",
    "        decoded_images_shifted_wo_sal = np.zeros(decoded_images.shape)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            decoded_images_shifted[i] = np.copy(np.roll(decoded_images[i], state_object.start_idx[i][1], axis=1))\n",
    "            decoded_images_shifted_wo_sal[i] = np.copy(np.roll(decoded_images[i], state_object.start_idx[i][1], axis=1))\n",
    "            \n",
    "        for t_view, visited_view in enumerate(visited_idxes):\n",
    "            for i in range(batch_size):                \n",
    "                curr_view = np.copy(state_object.views[i, visited_view[i][0], visited_view[i][1]])\n",
    "                curr_view = np.transpose(curr_view, (1, 2, 0))\n",
    "                # Modify the current view to have some borders (for easy visualization)\n",
    "                if t_view == t:\n",
    "                    if curr_view.shape[2] == 1:\n",
    "                        # If single channeled\n",
    "                        curr_view[:3, :, :] = 255 \n",
    "                        curr_view[-3:, :, :] = 255\n",
    "                        curr_view[:, :3, :] = 255\n",
    "                        curr_view[:, -3:, :] = 255\n",
    "                    else:\n",
    "                        # If 3 channeled\n",
    "                        curr_view[:3, :, 0] = 255\n",
    "                        curr_view[-3:, :, 0] = 255\n",
    "                        curr_view[:, :3, 0] = 255\n",
    "                        curr_view[:, -3:, 0] = 255\n",
    "                        curr_view[:3, :, 1:] = 0 \n",
    "                        curr_view[-3:, :, 1:] = 0\n",
    "                        curr_view[:, :3, 1:] = 0\n",
    "                        curr_view[:, -3:, 1:] = 0\n",
    "                    \n",
    "                curr_saliency = saliency_scores_all[iters][t_view][i]\n",
    "                curr_view_saliency = saliency_on_image(curr_saliency, curr_view, 1, channels=[0, 2], sigma=2)\n",
    "                decoded_images_shifted[i, visited_view[i][0], visited_view[i][1]] = \\\n",
    "                    np.transpose(curr_view_saliency, (2, 0, 1))\n",
    "                    \n",
    "                decoded_images_shifted_wo_sal[i, visited_view[i][0], visited_view[i][1]] = \\\n",
    "                    np.transpose(curr_view, (2, 0, 1))\n",
    "        \n",
    "        # Convert BxNxMxCx32x32 -> BxCx(32*N)x(32*M)\n",
    "        decoded_image_full_view = decoded_images_shifted.transpose((0, 3, 1, 4, 2, 5)).reshape(\\\n",
    "                                            batch_size, 1, opts.num_channels, opts.N*32, opts.M*32)\n",
    "        decoded_image_full_view_wo_sal = decoded_images_shifted_wo_sal.transpose((0, 3, 1, 4, 2, 5)).reshape(\\\n",
    "                                            batch_size, 1, opts.num_channels, opts.N*32, opts.M*32)\n",
    "        \n",
    "        decoded_all[iters].append(decoded_image_full_view)\n",
    "        decoded_all_raw[iters].append(decoded_image_full_view_wo_sal)\n",
    "        \n",
    "        # ============ Get the belief state saliency ============ \n",
    "        saliency_belief = []\n",
    "        for i in range(batch_size):\n",
    "            policy_input_curr = {}\n",
    "            # Get only current batch element\n",
    "            for k, v in policy_input.iteritems():\n",
    "                policy_input_curr[k] = v[i:i+1]\n",
    "            saliency_belief_curr, _ = get_belief_saliency(hidden_new[0][:, i:i+1], policy_input_curr, agent, opts.iscuda)\n",
    "            # roll saliency_belief_curr to reflect the unknown azimuth\n",
    "            saliency_belief_curr = np.roll(saliency_belief_curr, state_object.start_idx[i][1], axis=2)\n",
    "            saliency_belief_curr = saliency_belief_curr.transpose((0, 1, 3, 2, 4)).reshape(1, opts.N*32, opts.M*32)\n",
    "            saliency_belief.append(saliency_belief_curr)\n",
    "        \n",
    "        saliency_belief = np.concatenate(saliency_belief, axis=0)\n",
    "        saliency_belief_all[iters].append(saliency_belief)\n",
    "                        \n",
    "        # Act greedily\n",
    "        _, act = probs.max(dim=1)\n",
    "        act = act.data.view(-1, 1)\n",
    "        # Rotate the view\n",
    "        _ = state_object.rotate(act[:, 0])\n",
    "        \n",
    "        # Set hidden\n",
    "        hidden = hidden_new\n",
    "    \n",
    "    # Normalize the saliency values\n",
    "    for i in range(batch_size):        \n",
    "        saliency_max = 0\n",
    "        # Across time\n",
    "        #for t in range(opts.T):\n",
    "        #    saliency_curr = saliency_belief_all[iters][t][i]\n",
    "        #    curr_max = saliency_curr.max()\n",
    "        #    if curr_max > saliency_max:\n",
    "        #        saliency_max = curr_max\n",
    "        for t in range(opts.T):\n",
    "            # If normalizing only wrt time t\n",
    "            saliency_max = saliency_belief_all[iters][t][i].max()\n",
    "            # Normalize\n",
    "            saliency_belief_all[iters][t][i] /= (saliency_max + 1e-9)\n",
    "    \n",
    "    # Impose saliency on decoded views\n",
    "    for t in range(opts.T):\n",
    "\n",
    "        decoded_saliency_full_view = np.zeros((batch_size, 1, opts.num_channels, opts.N*32, opts.M*32))\n",
    "        for i in range(batch_size):\n",
    "            curr_saliency = saliency_belief_all[iters][t][i]\n",
    "            curr_view = decoded_all_raw[iters][t][i, 0].transpose(1, 2, 0)\n",
    "            decoded_saliency_full_view[i, 0] = saliency_on_image(curr_saliency, curr_view, 1, channels=[0, 2], sigma=2).transpose(2, 0, 1)\n",
    "        \n",
    "        decoded_saliency_all[iters].append(decoded_saliency_full_view)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize image saliency in numpy \n",
    "for iters in range(1):\n",
    "    batch_size = images_all[iters][0].shape[0]\n",
    "    images_count = 1\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        display_image = np.transpose(pano_all[iters][i][0], (1, 2, 0)).astype('float32')\n",
    "        display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "        fig = plt.figure(figsize=(12, 16))\n",
    "        if display_image.shape[2] == 1:\n",
    "            plt.imshow(display_image[:, :, 0])\n",
    "        else:\n",
    "            plt.imshow(display_image)\n",
    "        plt.show()\n",
    "        for t in range(opts.T):\n",
    "            #fig.add_subplot(batch_size*opts.T, 1, images_count)\n",
    "            fig = plt.figure(figsize=(12, 16))\n",
    "            display_image = np.transpose(decoded_all[iters][t][i][0], (1, 2, 0))\n",
    "            display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "            if display_image.shape[2] == 1:\n",
    "                plt.imshow(display_image[:, :, 0])\n",
    "            else:\n",
    "                plt.imshow(display_image)\n",
    "            images_count += 1\n",
    "            plt.show()\n",
    "            if opts.highres:\n",
    "                fig = plt.figure(figsize=(12, 16))\n",
    "                display_image = np.transpose(images_all_highres[iters][t][i], (1, 2, 0)).astype(np.float32)\n",
    "                display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "                if display_image.shape[2] == 1:\n",
    "                    plt.imshow(display_image[:, :, 0])\n",
    "                else:\n",
    "                    plt.imshow(display_image)\n",
    "                plt.show()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write outputs to tensorboard\n",
    "save_path = 'visualizations/vis_test/SUN360/multi_no_expert_reinforce/'\n",
    "writer = SummaryWriter(log_dir=save_path)\n",
    "images_count = 0\n",
    "for iters in range(num_iters):\n",
    "    batch_size = images_all[iters][0].shape[0]\n",
    "    for i in range(batch_size):\n",
    "        images_count += 1\n",
    "        outputs_all = []\n",
    "        display_image = np.transpose(pano_all[iters][i], (0, 2, 3, 1)).astype('float32')\n",
    "        display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "        outputs_all.append(display_image)\n",
    "        for t in range(opts.T):\n",
    "            display_image = np.transpose(decoded_all[iters][t][i], (0, 2, 3, 1))\n",
    "            display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "            outputs_all.append(display_image)\n",
    "            \n",
    "        outputs_all = np.concatenate(outputs_all, axis=0).transpose((0, 3, 1, 2))\n",
    "        x = vutils.make_grid(torch.Tensor(outputs_all), padding=5, normalize=True, scale_each=True, nrow=opts.T+1, pad_value=1.0)\n",
    "        writer.add_image('#%d Image Saliency'%(images_count), x, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize belief saliency in numpy\n",
    "for iters in range(1):\n",
    "    batch_size = decoded_saliency_all[iters][0].shape[0]\n",
    "    for i in range(batch_size):\n",
    "        for t in range(opts.T):\n",
    "            #pdb.set_trace()\n",
    "            display_image = np.transpose(decoded_saliency_all[iters][t][i][0], (1, 2, 0)).astype('float32')\n",
    "            display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "            fig = plt.figure(figsize=(12, 16))\n",
    "            if display_image.shape[2] == 1:\n",
    "                plt.imshow(display_image[:, :, 0])\n",
    "            else:\n",
    "                plt.imshow(display_image)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write outputs to tensorboard\n",
    "images_count = 0\n",
    "for iters in range(num_iters):\n",
    "    batch_size = decoded_saliency_all[iters][0].shape[0]\n",
    "    for i in range(batch_size):\n",
    "        images_count += 1\n",
    "        outputs_all = []\n",
    "        display_image = pano_all[iters][i].astype('float32')\n",
    "        display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "        outputs_all.append(display_image)\n",
    "        for t in range(opts.T):\n",
    "            display_image = decoded_saliency_all[iters][t][i].astype('float32')\n",
    "            display_image = (display_image - display_image.min())/(display_image.max()-display_image.min())\n",
    "            outputs_all.append(display_image)\n",
    "        \n",
    "        outputs_all = np.concatenate(outputs_all, axis=0)\n",
    "        x = vutils.make_grid(torch.Tensor(outputs_all), padding=5, normalize=True, scale_each=True, nrow=opts.T+1, pad_value=1.0)\n",
    "        writer.add_image('#%d Belief Saliency'%(images_count), x, 0)\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iters = 1\n",
    "loader.val_idx = 0\n",
    "for iters in range(num_iters):\n",
    "    if opts.expert_rewards:\n",
    "        pano, pano_rewards, depleted = loader.next_batch_val()\n",
    "        pano_maps = None\n",
    "    elif opts.expert_trajectories:\n",
    "        pano, pano_maps, depleted = loader.next_batch_val()\n",
    "        pano_rewards = None #TODO: Add expert rewards\n",
    "    else:\n",
    "        pano, depleted = loader.next_batch_val()\n",
    "        pano_rewards = None\n",
    "        pano_maps = None\n",
    "    \n",
    "    batch_size = opts.batch_size\n",
    "    start_idx = get_starts(opts.N, opts.M, batch_size, opts.start_view)\n",
    "    state_object = State(pano, pano_rewards, start_idx, opts)\n",
    "    hidden = None\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for t in range(opts.T):\n",
    "            print('============ t = %d ================='%(t))\n",
    "            # Get the original image's action distribution\n",
    "            \n",
    "            im_np, pro = state_object.get_view() # im_np - BxCx32x32\n",
    "            im, pro = preprocess(im_np, pro)\n",
    "\n",
    "            C = im.shape[1]\n",
    "            W = im.shape[2]\n",
    "            H = im.shape[3]\n",
    "\n",
    "            policy_input = {'im': im, 'pro': pro}\n",
    "            # Assuming only actOnElev\n",
    "            policy_input['elev'] = torch.Tensor([[state_object.idx[i][0]] for i in range(batch_size)])\n",
    "            policy_input['time'] = torch.Tensor([[t] for i in range(batch_size)])\n",
    "            if opts.iscuda:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = policy_input[var].cuda()\n",
    "\n",
    "            for var in policy_input:\n",
    "                policy_input[var] = Variable(policy_input[var])\n",
    "\n",
    "            probs, decoded, hidden_new, value = agent.policy.forward(policy_input, hidden)\n",
    "\n",
    "            # ============ Get the belief state saliency ============ \n",
    "            saliency_belief = []\n",
    "            for i in [b]:\n",
    "                print('Original panorama')\n",
    "                pano_copy = np.array(state_object.views[b])\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 0, -3:, :] = 255\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 0, :, -3:] = 255\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 0, :3, :] = 255\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 0, :, :3] = 255\n",
    "                \n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 1, -3:, :] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 1, :, -3:] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 1, :3, :] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 1, :, :3] = 0\n",
    "                \n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 2, -3:, :] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 2, :, -3:] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 2, :3, :] = 0\n",
    "                pano_copy[state_object.idx[b][0], state_object.idx[b][1], 2, :, :3] = 0\n",
    "                \n",
    "                plt.imshow(pano_copy.transpose((2, 0, 3, 1, 4)).reshape(\\\n",
    "                                            opts.num_channels, opts.N*32, opts.M*32)\\\n",
    "                                            .transpose(1, 2, 0))\n",
    "                plt.show()\n",
    "                for settings in [{'lr': 1e-4, 'iters': 2000}, {'lr': 1e-3, 'iters': 2000}, \\\n",
    "                                 {'lr': 1e-2, 'iters': 2000}, {'lr': 1e-1, 'iters': 2000}, \\\n",
    "                                 {'lr': 1e-4, 'iters': 20}, {'lr': 1e-3, 'iters': 20}, \\\n",
    "                                 {'lr': 1e-2, 'iters': 20}, {'lr': 1e-1, 'iters': 20}]:\n",
    "\n",
    "                    policy_input_curr = {}\n",
    "                    # Get only current batch element\n",
    "                    for k, v in policy_input.iteritems():\n",
    "                        policy_input_curr[k] = v[i:i+1]\n",
    "                    saliency_belief_curr, loss_value = get_belief_saliency(\\\n",
    "                                            hidden_new[0][:, i:i+1], policy_input_curr, \\\n",
    "                                            agent, opts.iscuda, lr=settings['lr'], \\\n",
    "                                            iters=settings['iters'])\n",
    "                    # roll saliency_belief_curr to reflect the unknown azimuth\n",
    "                    saliency_belief_curr = np.roll(saliency_belief_curr, state_object.start_idx[i][1], axis=2)\n",
    "                    saliency_belief_curr = saliency_belief_curr.transpose((0, 1, 3, 2, 4)).reshape(1, opts.N*32, opts.M*32)\n",
    "                    print('Settings: LR %.5f, Iters: %d, Loss: %f'%(settings['lr'], settings['iters'], loss_value))\n",
    "                    fig = plt.figure(figsize=(3, 4))\n",
    "                    plt.imshow(saliency_belief_curr[0])\n",
    "                    plt.show()\n",
    "                pdb.set_trace()\n",
    "\n",
    "            # Act greedily\n",
    "            _, act = probs.max(dim=1)\n",
    "            act = act.data.view(-1, 1)\n",
    "            # Rotate the view\n",
    "            _ = state_object.rotate(act[:, 0])\n",
    "\n",
    "            # Set hidden\n",
    "            hidden = hidden_new\n",
    "    #pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
